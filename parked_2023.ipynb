{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422385204faebe3b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Parked: Analysis of 2023 SFMTA Parking Meter and Parking Violation Data\n",
    "\n",
    "**Author:** Nick Gregorich\n",
    "**Date:** December, 2023\n",
    "\n",
    "This analysis was performed with 3 of my favorite tools:\n",
    "\n",
    "1. [DuckDB](https://duckdb.org): a wonderful analytics engine and SQL interface\n",
    "2. [pandas](https://pandas.pydata.org): the standard in Python data analysis\n",
    "3. [Plotly Express](https://plotly.com/python/plotly-express/): my current favorite interactive plotting library\n",
    "\n",
    "DuckDB allows us to query a [directory full of parquet files](https://duckdb.org/docs/data/parquet/overview) as if it were a table in a database with a SQL interface\n",
    "\n",
    "Let's import these 3 packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:21.721599Z",
     "start_time": "2024-02-18T23:34:19.296567Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/kv7k6n4x12dc6g6s548qsty00000gn/T/ipykernel_76917/1447275495.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe4df904953b77",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll need to set up a few user variables:\n",
    "\n",
    "1. `meter_data_dir`: location of the parking meter archives saved from: [DataSF](https://data.sfgov.org/Transportation/SFMTA-Parking-Meter-Detailed-Revenue-Transactions/imvp-dq3v/about_data)\n",
    "    1. Acquiring this data is left as an exercise for the reader\n",
    "2. `data_dir`: location of the parking violation archives saved from: [DataSF](https://data.sfgov.org/Transportation/SFMTA-Parking-Citations/ab4h-6ztd/about_data)\n",
    "    1. Acquiring this data is left as an exercise for the reader\n",
    "3. `large_amount_thresh`: some parking meter transactions are very large. It's unclear if these are data errors or maybe weekly or monthly passes. Either way, let's set a maximum transaction threshold to focus on what we think are probably single day transactions\n",
    "4. `negative_amount_thresh`: some parking meter transactions are negative. It's unclear if these are data errors or refunds. Either way, let's focus on positive transactions\n",
    "5. `sum_fines_min_thresh`: let's focus on parking violations that generated a total of over $1M in revenue over the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c706f4d2b43bef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:21.725396Z",
     "start_time": "2024-02-18T23:34:21.720692Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meter_data_dir = \"meter_data\"\n",
    "data_dir = \"ticket_data\"\n",
    "\n",
    "large_amount_thresh = 30\n",
    "negative_amount_thresh = 0\n",
    "\n",
    "sum_fines_min_thresh = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70b98b26002b7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The DuckDB [weekday()](https://duckdb.org/docs/sql/functions/datepart) function returns:\n",
    "\n",
    "```Numeric weekday synonym (Sunday = 0, Saturday = 6)```\n",
    "\n",
    "Let's create and show a *pandas DataFrame* to map the numeric weekday to the actual names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2ec57a341b8542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:21.729701Z",
     "start_time": "2024-02-18T23:34:21.724910Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         day\n",
       "0     Sunday\n",
       "1     Monday\n",
       "2    Tuesday\n",
       "3  Wednesday\n",
       "4   Thursday\n",
       "5     Friday\n",
       "6   Saturday"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_week_dict = {0: \"Sunday\", 1: \"Monday\", 2: \"Tuesday\", 3: \"Wednesday\", 4: \"Thursday\", 5: \"Friday\", 6: \"Saturday\"}\n",
    "\n",
    "day_week_df = pd.DataFrame.from_dict(day_week_dict, orient=\"index\", columns=[\"day\"])\n",
    "day_week_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4dcf17f13387a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll want to do a histogram of the parking meter values, but it can be nicer to define the bins where we want them instead of relying on the tool to set them automatically\n",
    "\n",
    "Let's create and show a *DataFrame* of bins with a $1 range (save for the top bin which should go to the max expected value)\n",
    "\n",
    "**Note:** in a future version, it would probably be wise to change the very last `upper` threshold from `10000.000` to `large_amount_thresh` for consistency and clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15527e4ce389928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:21.736254Z",
     "start_time": "2024-02-18T23:34:21.731651Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.01</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.01</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.01</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.01</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.01</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.01</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.01</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.01</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lower    upper\n",
       "0    0.00      1.0\n",
       "1    1.01      2.0\n",
       "2    2.01      3.0\n",
       "3    3.01      4.0\n",
       "4    4.01      5.0\n",
       "5    5.01      6.0\n",
       "6    6.01      7.0\n",
       "7    7.01      8.0\n",
       "8    8.01      9.0\n",
       "9    9.01     10.0\n",
       "10  10.01  10000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_bins_df = pd.DataFrame.from_dict(\n",
    "    {\"lower\": [0, 1.01, 2.01, 3.01, 4.01, 5.01, 6.01, 7.01, 8.01, 9.01, 10.01],\n",
    "     \"upper\": [1.00, 2.00, 3.00, 4.00, 5.00, 6.00, 7.00, 8.00, 9.00, 10.00, 10000.00]})\n",
    "\n",
    "hist_bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf27fed6064c0bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The violations in the dataset have a very abbreviated name\n",
    "\n",
    "Let's create and display a *DataFrame* that maps the abbreviated name into something a little more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7376626da578613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:21.740727Z",
     "start_time": "2024-02-18T23:34:21.735896Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>violation_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STR CLEAN</th>\n",
       "      <td>Street cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RES/OT</th>\n",
       "      <td>Residential parking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MTR OUT DT</th>\n",
       "      <td>Meter (out downtown)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METER DTN</th>\n",
       "      <td>Meter (downtown)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO PLATES</th>\n",
       "      <td>No plates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEL ZONE</th>\n",
       "      <td>Yellow zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAIL DISPL</th>\n",
       "      <td>Fare evasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIVEWAY</th>\n",
       "      <td>Blocking driveway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PK PHB OTD</th>\n",
       "      <td>No parking (not downtown)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRK PROHIB</th>\n",
       "      <td>No parking (downtown)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DBL PARK</th>\n",
       "      <td>Double parking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUS ZONE</th>\n",
       "      <td>Bus zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON SIDEWALK</th>\n",
       "      <td>On sidewalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RED ZONE</th>\n",
       "      <td>Red zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRK ZONE</th>\n",
       "      <td>Truck loading zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO EV REG</th>\n",
       "      <td>No registration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRK GRADE</th>\n",
       "      <td>Parking grade</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        violation_text\n",
       "STR CLEAN              Street cleaning\n",
       "RES/OT             Residential parking\n",
       "MTR OUT DT        Meter (out downtown)\n",
       "METER DTN             Meter (downtown)\n",
       "NO PLATES                    No plates\n",
       "YEL ZONE                   Yellow zone\n",
       "FAIL DISPL                Fare evasion\n",
       "DRIVEWAY             Blocking driveway\n",
       "PK PHB OTD   No parking (not downtown)\n",
       "PRK PROHIB       No parking (downtown)\n",
       "DBL PARK                Double parking\n",
       "BUS ZONE                      Bus zone\n",
       "ON SIDEWALK                On sidewalk\n",
       "RED ZONE                      Red zone\n",
       "TRK ZONE            Truck loading zone\n",
       "NO EV REG              No registration\n",
       "PRK GRADE                Parking grade"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violation_dict = {\n",
    "    \"STR CLEAN\": \"Street cleaning\",\n",
    "    \"RES/OT\": \"Residential parking\",\n",
    "    \"MTR OUT DT\": \"Meter (out downtown)\",\n",
    "    \"METER DTN\": \"Meter (downtown)\",\n",
    "    \"NO PLATES\": \"No plates\",\n",
    "    \"YEL ZONE\": \"Yellow zone\",\n",
    "    \"FAIL DISPL\": \"Fare evasion\",\n",
    "    \"DRIVEWAY\": \"Blocking driveway\",\n",
    "    \"PK PHB OTD\": \"No parking (not downtown)\",\n",
    "    \"PRK PROHIB\": \"No parking (downtown)\",\n",
    "    \"DBL PARK\": \"Double parking\",\n",
    "    \"BUS ZONE\": \"Bus zone\",\n",
    "    \"ON SIDEWALK\": \"On sidewalk\",\n",
    "    \"RED ZONE\": \"Red zone\",\n",
    "    \"TRK ZONE\": \"Truck loading zone\",\n",
    "    \"NO EV REG\": \"No registration\",\n",
    "    \"PRK GRADE\": \"Parking grade\",\n",
    "}\n",
    "\n",
    "violation_df = pd.DataFrame.from_dict(violation_dict, orient=\"index\", columns=[\"violation_text\"])\n",
    "violation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95ad362b1088cf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It's finally time to dive into the actual data!\n",
    "\n",
    "First, we run a SQL `describe` on the parking meter data in order to see its schema\n",
    "\n",
    "In the DuckDB workflow, we:\n",
    "\n",
    "1. Define the DuckDB SQL query in a Python (f-)`string`\n",
    "   1. Instead of a table name, use the [read_parquet](https://duckdb.org/docs/data/parquet/overview) function\n",
    "   2. Point `read_parquet` to the directory containing the parking meter data\n",
    "   3. Use a [glob](https://en.wikipedia.org/wiki/Glob_%28programming%29) to specify all the `.parquet` files in the directory\n",
    "2. Run the query and use the `.to_df()` method to convert the result of the query to a pandas DataFrame\n",
    "3. Display the DataFrame\n",
    "\n",
    "**Note:** the documentation refers to `.df()` rather than `.to_df()`, so later versions may have simplified this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee40fa525a361c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:21.794640Z",
     "start_time": "2024-02-18T23:34:21.738471Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_type</th>\n",
       "      <th>null</th>\n",
       "      <th>key</th>\n",
       "      <th>default</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transmission_datetime</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post_id</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>street_block</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>payment_type</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>session_start_dt</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>session_end_dt</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meter_event_type</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gross_paid_amt</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column_name column_type null   key default extra\n",
       "0  transmission_datetime     VARCHAR  YES  None    None  None\n",
       "1                post_id     VARCHAR  YES  None    None  None\n",
       "2           street_block     VARCHAR  YES  None    None  None\n",
       "3           payment_type     VARCHAR  YES  None    None  None\n",
       "4       session_start_dt     VARCHAR  YES  None    None  None\n",
       "5         session_end_dt     VARCHAR  YES  None    None  None\n",
       "6       meter_event_type     VARCHAR  YES  None    None  None\n",
       "7         gross_paid_amt     VARCHAR  YES  None    None  None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"describe select * from read_parquet('{meter_data_dir}/*.parquet');\"\n",
    "columns_df = duckdb.sql(query).to_df()\n",
    "columns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b066548476cc02d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are 8 [varchar](https://en.wikipedia.org/wiki/Varchar) columns. We can use a standard `select * from table limit 1` query to inspect a single row of the data and learn what the columns contain\n",
    "\n",
    "**Note:** Since every column is a `varchar` *string* instead of a specialized type like `timestamp` or `double`, we need to `cast()` the varchar to the desired type in every query\n",
    " \n",
    "I expect this to result in a performance hit since we are likely doing the same `cast()` repeatedly on the same data. We could `cast()` once and save the data to memory or disk, but that also may be prohibitive depending on the size of the dataset and resources available\n",
    " \n",
    "The performance of `cast()` in every query was sufficient for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761f5039f8f9f9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:22.190462Z",
     "start_time": "2024-02-18T23:34:21.745995Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transmission_datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>street_block</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>session_start_dt</th>\n",
       "      <th>session_end_dt</th>\n",
       "      <th>meter_event_type</th>\n",
       "      <th>gross_paid_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112784135_4_01012017000836</td>\n",
       "      <td>585-02240</td>\n",
       "      <td>NORTH POINT ST 200</td>\n",
       "      <td>CASH</td>\n",
       "      <td>2017-01-01T00:08:36.000</td>\n",
       "      <td>2017-01-01T00:08:36.000</td>\n",
       "      <td>NS</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        transmission_datetime    post_id        street_block payment_type  \\\n",
       "0  112784135_4_01012017000836  585-02240  NORTH POINT ST 200         CASH   \n",
       "\n",
       "          session_start_dt           session_end_dt meter_event_type  \\\n",
       "0  2017-01-01T00:08:36.000  2017-01-01T00:08:36.000               NS   \n",
       "\n",
       "  gross_paid_amt  \n",
       "0            0.5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"select * from read_parquet('{meter_data_dir}/*.parquet') limit 1;\"\n",
    "a_row_df = duckdb.sql(query).to_df()\n",
    "a_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfb0cbff43425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can see what we're dealing with in the parking meter data! Note that the DataSF link above has simple descriptions of the columns\n",
    "\n",
    "1. `transmission_datetime = 1044369491_9_01032023131055`\n",
    "    1. Using context clues from other columns, we can determine\n",
    "    2. The 3rd section of the string seems to start with a date\n",
    "        1. `01` month\n",
    "        2. `03` day\n",
    "        3. `2023` year\n",
    "    3. Then there's a time\n",
    "        1. `13` hour\n",
    "        2. `10` minute\n",
    "        3. `55` second\n",
    "    4. The first section of the string may be more of a transaction number?\n",
    "        1. If so, this column looks like the only unique identifier\n",
    "2. `post_id = 722-28260`\n",
    "    1. This is a unique identifier for the parking meter\n",
    "    2. We may want to `group by` this column to compare revenue of different parking meters\n",
    "3. `street_block = WEBSTER ST 2800`\n",
    "    1. This is the street name and block of the parking meter\n",
    "4. `payment_type = PAY BY CELL`\n",
    "    1. Obviously this is the payment type of the transaction\n",
    "    2. We may want to do a `group by` on this column to compare revenue from different payment methods\n",
    "5. `session_start_dt = 2023-01-03T13:10:55.000`\n",
    "    1. The starting datetime of the parking meter session\n",
    "    2. Note that this is the same datetime we see in the first column, `transmission_datetime`\n",
    "6. `session_end_dt = 2023-01-03T14:10:55.000`\n",
    "    1. The ending datetime of the parking meter session\n",
    "    2. This would have been nice to compare the durations of parking meter payments\n",
    "    3. However, this column was never used in analysis\n",
    "7. `meter_event_type = NS`\n",
    "    1. Parking meter event type including:\n",
    "        1. `NS = new session`\n",
    "        2. `AT = additional time`\n",
    "        3. `SC = status change`\n",
    "    2. This column was not used in analysis\n",
    "8. `gross_paid_amt = 6.75`\n",
    "    1. Amount paid to the parking meter by the customer\n",
    "    2. This column is the most interesting and is the basis for most of the analysis\n",
    "\n",
    "Cool, we have a much better understanding of the parking meter data!\n",
    "\n",
    "Let's see the datetime range of the dataset. We'll look at:\n",
    "\n",
    "`min(session_start_dt::timestamp) as first_start_ts`, the minimum session starting datetime as the beginning of the dataset and\n",
    "\n",
    "`max(session_start_dt::timestamp) as last_start_ts`, the maximum session starting datetime as the end of the dataset\n",
    "\n",
    "**Note:** notice the `::timestamp` cast as mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f02f61505f727cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:23.552530Z",
     "start_time": "2024-02-18T23:34:22.193014Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_start_ts</th>\n",
       "      <th>last_start_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:08:36</td>\n",
       "      <td>2023-12-16 22:38:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       first_start_ts       last_start_ts\n",
       "0 2017-01-01 00:08:36 2023-12-16 22:38:27"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"select\n",
    "min(session_start_dt::timestamp) as first_start_ts,\n",
    "max(session_start_dt::timestamp) as last_start_ts\n",
    "from read_parquet('{meter_data_dir}/*.parquet');\"\"\"\n",
    "first_last_start_df = duckdb.sql(query).to_df()\n",
    "first_last_start_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78270eb8f4262300",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The dataset started on January 1st, 2017 at 12:08:36 AM and the most recent entry as of this writing is December 16th, 2023 at 10:38:27 PM\n",
    "\n",
    "Nearly 6 years, that's a lot of parking meter transactions!\n",
    "\n",
    "But how many? Great question, let's find out by counting the number of rows / entries in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3daaddf53d790af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:23.565547Z",
     "start_time": "2024-02-18T23:34:23.551958Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167449468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_rows\n",
       "0   167449468"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"select count(*) as count_rows from read_parquet('{meter_data_dir}/*.parquet');\"\n",
    "count_rows_df = duckdb.sql(query).to_df()\n",
    "count_rows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7034f64cc7534ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wow, there are 167,449,468 parking meter transactions in the 6 years of records! If we assume a constant number of transactions per year, that would be over 27 million transactions per year\n",
    "\n",
    "One of the most important (and least glorious) aspects of working with data is *cleaning* it. We can try and learn a little bit about the data and combine it with our intuition in order to know which data is trustworthy and which is not. This can be things like duplicate data, missing data, or outliers\n",
    "\n",
    "We might expect all parking meter transactions to be positive since we generally don't get paid *by* meters to park, so let's see if there are any negative transaction amounts and if so, how many\n",
    "\n",
    "**Note:** notice the `gross_paid_amt` column is cast to `double`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a903f9727ab9be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:24.544622Z",
     "start_time": "2024-02-18T23:34:23.565476Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_rows\n",
       "0           8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"select count(gross_paid_amt) as count_rows\n",
    "from read_parquet('{meter_data_dir}/*.parquet')\n",
    "where gross_paid_amt::double < {negative_amount_thresh};\"\"\"\n",
    "count_negative_amount_df = duckdb.sql(query).df()\n",
    "count_negative_amount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34ceb36cacc4f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are only 8 (out of 167 million) records that have negative dollar amounts. That's a very small percentage, so it is unlikely to meaningfully affect our analysis (we could look at the actual amounts to estimate their affect), but we'll still filter them out\n",
    "\n",
    "We might expect parking meter transaction amounts to have a reasonable upper limit. We chose a threshold of $30 in the user variables section above, which feels like it should cover the range *normal* parking meter transactions\n",
    "\n",
    "Let's see how many transactions are greater than our upper threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc105331b50a807c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:25.558796Z",
     "start_time": "2024-02-18T23:34:24.543688Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_rows\n",
       "0      145363"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"select count(gross_paid_amt) as count_rows\n",
    "from read_parquet('{meter_data_dir}/*.parquet')\n",
    "where gross_paid_amt::double > {large_amount_thresh};\"\"\"\n",
    "count_large_amount_df = duckdb.sql(query).df()\n",
    "count_large_amount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6421bebc2fe2b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are 145,363 transactions over $30 in the dataset, significantly more than the negative transactions! It would be nice to understand these better, but for this analysis we'll just filter them out\n",
    "\n",
    "Let's focus on transactions in 2023 and see how many there were. We'll:\n",
    "\n",
    "1. Cast the session start `varchar` to a `date`\n",
    "2. Take the `year()` of the `date`\n",
    "3. Filter where `year()` is 2023\n",
    "4. Filter where transactions are between 0 and 30 (inclusive)\n",
    "5. `group by` the year so we can `count()` the number of payments\n",
    "    1. In a future version, I would put `year(session_start_dt::date)` in the `where` so I could drop the `group by`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23f39259c7a1a32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:27.184182Z",
     "start_time": "2024-02-18T23:34:25.559613Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>count_payments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>17478748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  count_payments\n",
       "0  2023        17478748"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "select year(session_start_dt::date) as year, count(gross_paid_amt) as count_payments\n",
    "from read_parquet('{meter_data_dir}/*.parquet') as data_t\n",
    "where year = 2023\n",
    "and gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "group by year;\n",
    "\"\"\"\n",
    "payment_count_2023_df = duckdb.sql(query).to_df()\n",
    "payment_count_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caa8bc8f1c2e97",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There were 17,478,748 parking meter transactions in 2023. Although the year wasn't quite over, that is significantly lower than the 27 million transactions per year we estimated above\n",
    "\n",
    "How much revenue did these 17 million transactions generate?\n",
    "\n",
    "1. Start with the previous query\n",
    "2. Instead of counting the number of transaction amounts\n",
    "    1. Sum the amount paid (cast to `double`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "276a93e2caa04402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:28.966300Z",
     "start_time": "2024-02-18T23:34:27.183709Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sum_amount_paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>4.727701e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  sum_amount_paid\n",
       "0  2023     4.727701e+07"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "select year(session_start_dt::date) as year, sum(gross_paid_amt::double) as sum_amount_paid\n",
    "from read_parquet('{meter_data_dir}/*.parquet') as data_t\n",
    "where year = 2023\n",
    "and gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "group by year;\n",
    "\"\"\"\n",
    "amount_sum_2023_df = duckdb.sql(query).to_df()\n",
    "amount_sum_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd1439637e9bf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wow, over $47M was collected in parking meter transactions! That's a solid business!\n",
    "\n",
    "Above, we noted that the payment method is present in the table. Let's take a look by doing a `group by` on `payment_type` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "350ebfeae8282d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:31.360023Z",
     "start_time": "2024-02-18T23:34:28.952876Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_type</th>\n",
       "      <th>sum_amount_paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASH</td>\n",
       "      <td>3.631406e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREDIT CARD</td>\n",
       "      <td>2.822013e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAY BY CELL</td>\n",
       "      <td>1.520821e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMART CARD</td>\n",
       "      <td>2.172594e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  payment_type  sum_amount_paid\n",
       "0         CASH     3.631406e+06\n",
       "1  CREDIT CARD     2.822013e+07\n",
       "2  PAY BY CELL     1.520821e+07\n",
       "3   SMART CARD     2.172594e+05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "select payment_type, sum(gross_paid_amt::double) as sum_amount_paid\n",
    "from read_parquet('{meter_data_dir}/*.parquet') as data_t\n",
    "where year(session_start_dt::date) = 2023\n",
    "and gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "group by payment_type\n",
    "order by payment_type\n",
    "\"\"\"\n",
    "amount_type_2023_df = duckdb.sql(query).to_df()\n",
    "amount_type_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6bb51a4d1b1b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It looks like there are 4 payment methods in the dataset:\n",
    "\n",
    "1. Cash: &#36;3.6M\n",
    "2. Credit card: &#36;28M\n",
    "3. Pay by cell phone: &#36;15M\n",
    "4. Smart card: &#36;217k\n",
    "\n",
    "Credit cards were the most common payment method at &#36;28M followed by pay by cell phone around half that at &#36;15M. Cash was an order of magnitude lower at &#36;3.6M and a legacy payment method called smart card was another order of magnitude lower at &#36;217k\n",
    "\n",
    "If we get these numbers as a fraction of the total, we can build a nice pie or donut chart for our infographic!\n",
    "\n",
    "Here we do some manipulation of the *pandas DataFrame* saved above in order to generate the fractions of the whole (aka percentages). In hindsight this was a little bit of a strange decision since we did all of our previous data manipulation in *DuckDB* / *SQL*\n",
    "\n",
    "1. Create a Python dictionary with keys of payment method and the aggregate sum of the payment amounts\n",
    "    1. For simplicity, the legacy smart cards are lumped in with credit cards\n",
    "2. Create a *pandas DataFrame* from the dictionary, rename and reindex\n",
    "3. Create a new `fraction` column in the DataFrame\n",
    "4. Set the `fraction` column to the amount paid per transaction method over the total of all transaction amounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497237047b8f25a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:31.369357Z",
     "start_time": "2024-02-18T23:34:31.364327Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sum_amount_paid</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASH</td>\n",
       "      <td>3.631406e+06</td>\n",
       "      <td>CASH</td>\n",
       "      <td>0.076811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREDIT CARD</td>\n",
       "      <td>2.843739e+07</td>\n",
       "      <td>CREDIT CARD</td>\n",
       "      <td>0.601506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAY BY CELL</td>\n",
       "      <td>1.520821e+07</td>\n",
       "      <td>PAY BY CELL</td>\n",
       "      <td>0.321683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  sum_amount_paid payment_type  fraction\n",
       "0         CASH     3.631406e+06         CASH  0.076811\n",
       "1  CREDIT CARD     2.843739e+07  CREDIT CARD  0.601506\n",
       "2  PAY BY CELL     1.520821e+07  PAY BY CELL  0.321683"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_type_2023_dict = {\n",
    "    \"CASH\": amount_type_2023_df[amount_type_2023_df.payment_type == \"CASH\"].iloc[0].sum_amount_paid,\n",
    "    \"CREDIT CARD\": amount_type_2023_df[amount_type_2023_df.payment_type == \"CREDIT CARD\"].iloc[0].sum_amount_paid + \\\n",
    "                   amount_type_2023_df[amount_type_2023_df.payment_type == \"SMART CARD\"].iloc[0].sum_amount_paid,\n",
    "    \"PAY BY CELL\": amount_type_2023_df[amount_type_2023_df.payment_type == \"PAY BY CELL\"].iloc[0].sum_amount_paid,\n",
    "}\n",
    "amount_type_2023_df = pd.DataFrame.from_dict(amount_type_2023_dict, orient=\"index\")\n",
    "amount_type_2023_df = amount_type_2023_df.rename(columns={0: \"sum_amount_paid\"})\n",
    "amount_type_2023_df[\"payment_type\"] = amount_type_2023_df.index\n",
    "amount_type_2023_df = amount_type_2023_df.reset_index()\n",
    "amount_type_2023_df[\"fraction\"] = amount_type_2023_df.sum_amount_paid / amount_type_2023_df.sum_amount_paid.sum()\n",
    "amount_type_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52e825e4d11fa8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nice, we can see credit card was 60% of transactions by dollar amount, pay by cell phone was 32% by amount, and cash was just 7.7% by amount\n",
    "\n",
    "Let's see a pie chart of this data using *Plotly Express*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e1c7ae2ce33a14d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:31.700126Z",
     "start_time": "2024-02-18T23:34:31.368270Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mpie(amount_type_2023_df, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_amount_paid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_traces(textposition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minside\u001b[39m\u001b[38;5;124m'\u001b[39m, textinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/sf_meters/parked/.venv/lib/python3.12/site-packages/plotly/basedatatypes.py:3410\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/sf_meters/parked/.venv/lib/python3.12/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "fig = px.pie(amount_type_2023_df, values=\"sum_amount_paid\")\n",
    "fig.update_traces(textposition='inside', textinfo='none')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0ffe2c1c51ea9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nice, the first chart for our infographic! Thanks Plotly!\n",
    "\n",
    "It would be interesting to know some details about the parking meter transaction amounts. Are the bulk of the transactions $1 quick stops or are they higher dollar, presumably longer duration events?\n",
    "\n",
    "This will be our most ambitious query yet:\n",
    "\n",
    "1. The `from` will be the same `read_parquet()` *table* we've been using\n",
    "2. `cross join` to the `hist_bins_df` DataFrame we created above\n",
    "    1. `cross join` is a rare join method but possibly the simplest: it returns all the possible pairing between both tables\n",
    "    2. `hist_bins_df` is the DataFrame of histogram ranges we created above\n",
    "        1. It contains a lower and upper limit (inclusive) for each bin we want in our histogram\n",
    "3. `select` the `lower` and `upper` limits from `hist_bins_df`\n",
    "4. `group by` the same `lower` and `upper` limits so we can do an aggregate count of the number of parking meter payments\n",
    "5. Use the same filters as before\n",
    "    1. Year is 2023\n",
    "    2. Transaction amount is between 0 and 30 (inclusive)\n",
    "6. `order by` the upper limit\n",
    "7. After the *DuckDB SQL* query\n",
    "    1. Use pandas to add a `fraction` of each bin of the total number of transactions\n",
    "    2. Use pandas to add a `proportion` of each fraction relative to the max fraction\n",
    "        1. In our infographic, we may want to make relatively sized objects. The `proportion` column will help there\n",
    "\n",
    "**Note:** in a future version, I would order `lower` before `upper`. We also don't need to do an aggregate count of the `gross_paid_amt` `cast()` to `double`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c4e312d5b6035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:37.586498Z",
     "start_time": "2024-02-18T23:34:31.640634Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select hist_bins_df.upper, hist_bins_df.lower, count(gross_paid_amt::double) as count_rows\n",
    "from read_parquet('{meter_data_dir}/*.parquet')\n",
    "cross join hist_bins_df\n",
    "where year(session_start_dt::date) = 2023\n",
    "and gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "and gross_paid_amt::double between hist_bins_df.lower and hist_bins_df.upper\n",
    "group by hist_bins_df.lower, hist_bins_df.upper\n",
    "order by hist_bins_df.upper\n",
    "\"\"\"\n",
    "amount_2023_hist_df = duckdb.sql(query).to_df()\n",
    "amount_2023_hist_df[\"fraction\"] = amount_2023_hist_df.count_rows / amount_2023_hist_df.count_rows.sum()\n",
    "amount_2023_hist_df[\"proportion\"] = amount_2023_hist_df.fraction / amount_2023_hist_df.fraction.max()\n",
    "amount_2023_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88749e8fe3efe98e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's a nice table of results!\n",
    "\n",
    "1. In the first 2 columns, we see the range of each histogram bin\n",
    "2. In the `count_rows` column, we see the number of transactions that fall into each bin\n",
    "3. In the `fraction` column, we see each bin's / row's fraction of the total number of transactions\n",
    "4. In the `proportion` column, we see each row's `fraction` normalized by the largest `fraction`\n",
    "\n",
    "The greatest number of transactions was in the &#36;0 - &#36;1 range, with over 6.8 million transactions which is over 39% of the total number of transactions. Note that the proportion is 1.0 here\n",
    "\n",
    "The number of transactions in the &#36;1 - &#36;2 range is half of the &#36;0 - &#36;1. Numbers decrease as you may expect until the &#36;10 - &#36;10,000 range. This makes sense, the bin is very large\n",
    "\n",
    "**Note:** as mentioned previously, it was confusing to have the top bin go to &#36;10,000 when the query is filtering to &#36;30\n",
    "\n",
    "Another thing we could look at is the amount of parking meter revenue by day of the week. We can:\n",
    "\n",
    "1. Cast the parking session start datetime to `date`, then use the `weekday()` function to extract the day of the week\n",
    "2. `group by` the day of the week so we can aggregate the sum of the amount paid (cast to `double`)\n",
    "3. Calculate the fraction and proportion in pandas, as we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b4586a8fd4c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:39.639603Z",
     "start_time": "2024-02-18T23:34:37.574437Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select weekday(session_start_dt::date) as day_week, sum(gross_paid_amt::double) as sum_amount_paid\n",
    "from read_parquet('{meter_data_dir}/*.parquet')\n",
    "where year(session_start_dt::date) = 2023\n",
    "and gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "group by day_week;\n",
    "\"\"\"\n",
    "amount_day_hist_df = duckdb.sql(query).to_df()\n",
    "amount_day_hist_df = amount_day_hist_df.merge(day_week_df, left_on=\"day_week\", right_index=True)\n",
    "amount_day_hist_df[\"fraction\"] = amount_day_hist_df.sum_amount_paid / amount_day_hist_df.sum_amount_paid.sum()\n",
    "amount_day_hist_df[\"proportion\"] = amount_day_hist_df.fraction / amount_day_hist_df.fraction.max()\n",
    "amount_day_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6fb118bc0e625",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Interesting results, we see fairly similar revenue (and therefore fraction) per day -- except on Sunday! This makes sense, many parking meters do not require payment on Sunday. It's unclear if some parking meters do in fact collect on Sunday or if people mistakenly pay on a day that is free. Sunday still has a respectable revenue of over $1M\n",
    "\n",
    "Next we could look at is the amount of parking meter revenue by hour of the day. We can take the last query and substitute the `weekday()` function with `hour()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855993268b57f15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:41.417109Z",
     "start_time": "2024-02-18T23:34:39.639879Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select hour(session_start_dt::timestamp) as hour, sum(gross_paid_amt::double) as sum_amount_paid\n",
    "from read_parquet('{meter_data_dir}/*.parquet')\n",
    "where year(session_start_dt::date) = 2023\n",
    "and gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "group by hour;\n",
    "\"\"\"\n",
    "amount_hour_hist_df = duckdb.sql(query).to_df()\n",
    "amount_hour_hist_df[\"fraction\"] = amount_hour_hist_df.sum_amount_paid / amount_hour_hist_df.sum_amount_paid.sum()\n",
    "amount_hour_hist_df[\"proportion\"] = amount_hour_hist_df.fraction / amount_hour_hist_df.fraction.max()\n",
    "amount_hour_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4edae2dc669990",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This looks like it will make a nice visualization!\n",
    "\n",
    "That's interesting, the highest revenue by hour is at noon: folks getting lunch and / or running errands at lunch? There are relative maxima at 9 AM, 11 AM, and 1 PM too\n",
    "\n",
    "Maybe we could find the highest grossing parking meters to get an idea of where the hotspots are around town\n",
    "\n",
    "1. `group by` the `street_block` column\n",
    "2. Aggregate the sum of the amount paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801ccb876df7bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:43.951041Z",
     "start_time": "2024-02-18T23:34:41.417096Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select street_block, sum(gross_paid_amt::double) as sum_amount_paid\n",
    "from read_parquet('{meter_data_dir}/*.parquet') as data_t\n",
    "where gross_paid_amt::double between {negative_amount_thresh} and {large_amount_thresh}\n",
    "and year(session_start_dt::date) = 2023\n",
    "group by street_block\n",
    "order by sum_amount_paid desc;\n",
    "\"\"\"\n",
    "amount_meter_2023_df = duckdb.sql(query).to_df()\n",
    "amount_meter_2023_df[\"fraction\"] = amount_meter_2023_df.sum_amount_paid / amount_meter_2023_df.sum_amount_paid.sum()\n",
    "amount_meter_2023_df[\"proportion\"] = amount_meter_2023_df.fraction / amount_meter_2023_df.fraction.max()\n",
    "amount_meter_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad784c6c024fd8bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nice, we have a list of 1,743 parking meters, the total revenue in 2023 per meter, the fraction of the total revenue per meter, and the proportion normalized to the maximum revenue per meter\n",
    "\n",
    "We could use a plot to visualize the revenue per parking meter to get an idea of the distribution: are all meters created equal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd13c48b151146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:44.325972Z",
     "start_time": "2024-02-18T23:34:43.950826Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = px.line(amount_meter_2023_df, y=\"sum_amount_paid\",\n",
    "              title=f\"Amount paid per SF parking meter in 2023\")\n",
    "fig.update_layout(xaxis_title='Meter number')\n",
    "fig.update_yaxes(title_text='Total ($)')\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91f296aee15e76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To answer our own question, no, not all parking meters are created equal. A small fraction of parking meters generate on the order of a quarter million dollars a year, the vast majority seem to collect in the &#36;10,000 to &#36;100,000 range, and a sizeable tail collect less than &#36;10,000 a year\n",
    "\n",
    "Let's see the max revenue from a parking meter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea303cd52417f668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:44.329834Z",
     "start_time": "2024-02-18T23:34:43.992901Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_meter_2023_df.sum_amount_paid.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eba607f0b47e00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&#36;266,266 from a single parking meter, wow!\n",
    "\n",
    "Let's see the median revenue collected from a meter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28d72d80a05d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:44.335348Z",
     "start_time": "2024-02-18T23:34:43.995101Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_meter_2023_df.sum_amount_paid.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cdfc6a3457ee1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The median is &#36;16.5k, a far cry from the maximum value!\n",
    "\n",
    "OK, let's take a look at a different dataset from DataSF: parking citations\n",
    "\n",
    "We can go a little bit quicker this time. First, we'll inspect the columns of the dataset with a `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6dabf39bfe763b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:44.336649Z",
     "start_time": "2024-02-18T23:34:43.997667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"describe select * from read_parquet('{data_dir}/*.parquet');\"\n",
    "columns_df = duckdb.sql(query).to_df()\n",
    "columns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997269481156ab64",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This dataset has 10 columns, also all `varchar` strings. Let's query a single row so we can get familiar with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d8b5f2026447b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:44.416234Z",
     "start_time": "2024-02-18T23:34:44.006373Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"select * from read_parquet('{data_dir}/*.parquet') limit 1;\"\n",
    "a_row_df = duckdb.sql(query).to_df()\n",
    "a_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5610cd6b4da670b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's look at this one in detail:\n",
    "\n",
    "1. `citation_number = 848824174`\n",
    "    1. This one's pretty straight forward\n",
    "    2. We expect it to be a unique identifier\n",
    "2. `citation_issued_datetime = 2014-11-26T08:10:00.000`\n",
    "    1. Again, probably straight forward\n",
    "3. `violation = TRC7.2.22`\n",
    "    1. Violation code as found [here](https://www.sfmta.com/sites/default/files/reports-and-documents/2022/05/fy_2023_fees_and_fines_effective_7.1.22.pdf)\n",
    "4. `violation_desc = STR CLEAN`\n",
    "    1. A slightly more detailed but still fairly short description of the violation\n",
    "    2. A longer description is available [here](https://www.sfmta.com/sites/default/files/reports-and-documents/2022/05/fy_2023_fees_and_fines_effective_7.1.22.pdf)\n",
    "5. `citation_location = 799 SHRADER ST`\n",
    "    1. Straight forward\n",
    "    2. Might be interesting to look at violations within a radius to find citation hot spots\n",
    "        1. This is not included in this analysis\n",
    "6. `vehicle_state_plate = CA`\n",
    "    1. Straight forward\n",
    "7. `vehicle_plate = 4SWC745`\n",
    "    1. Straight forward\n",
    "    2. We could aggregate the sum by `vehicle_plate` to find the offenders with the highest fines per year\n",
    "8. `fine_amount = 66`\n",
    "    1. Straight forward\n",
    "    2. Probably the basis for most of the analysis of this dataset\n",
    "9. `date_added = 2014-11-26T00:00:00.000`\n",
    "    1. Date added to dataset (according to DataSF)\n",
    "    2. We probably won't use this data\n",
    "10. `the_geom = None`\n",
    "    1. Geojson point of citation location (according to DataSF)\n",
    "    2. I believe this is rarely populated, so we can ignore it\n",
    "\n",
    "**Note:** this data was not originally checked for duplicates and unfortunately there are duplicates. To de-duplicate the data, we could do a big `group by` and select the lowest (assumed to be original) `date_added`. Something like:\n",
    "\n",
    "```sql\n",
    "select\n",
    "    citation_number,\n",
    "    citation_issued_datetime,\n",
    "    violation,\n",
    "    violation_desc,\n",
    "    citation_location,\n",
    "    vehicle_state_plate,\n",
    "    vehicle_plate,\n",
    "    fine_amount,\n",
    "    min(date_added) as original_date_added,\n",
    "    the_geom\n",
    " from\n",
    "    read_parquet('{data_dir}/*.parquet')\n",
    " group by\n",
    "    citation_number,\n",
    "    citation_issued_datetime,\n",
    "    violation,\n",
    "    violation_desc,\n",
    "    citation_location,\n",
    "    vehicle_state_plate,\n",
    "    vehicle_plate,\n",
    "    fine_amount,\n",
    "    the_geom\n",
    "```\n",
    "\n",
    "Similar to the parking meter data, we can get the sum of all the fines in 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8ffdd066922b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T23:34:44.418276Z",
     "start_time": "2024-02-18T23:34:44.341760Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\\query = f\"\"\"select sum(fine_amount::double) as sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023;\"\"\"\n",
    "fines_sum_2023_df = duckdb.sql(query).to_df()\n",
    "fines_sum_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ab54eb68a989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wow, over $99M in parking violations in 2023 alone! I wonder how many violations that was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0ac3f4a51fe47",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.344875Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select count(fine_amount) as count_rows\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023;\"\"\"\n",
    "fines_count_2023_df = duckdb.sql(query).to_df()\n",
    "fines_count_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03202ba7caafb01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There were 1,014,481 citations. A quick spot check is &#36;99M / 1M citations = &#36;99 per citation. That seems about right\n",
    "\n",
    "In San Francisco, many streets have a posted street cleaning time window where parking is not allowed that can be once a week or even every day downtown. It would probably be interesting to see a breakdown of total fine amounts for street cleaning versus all other violations. We can:\n",
    "\n",
    "1. Combine 2 queries with a `union`\n",
    "    1. A query summing the fines for street cleaning violations in 2023\n",
    "    2. A query summing the fines for violations that are not street cleaning in 2023\n",
    "2. Calculate the fraction in pandas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371a636e43b72f7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.346179Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select 'Street cleaning' as violation_desc, sum(fine_amount::double) as sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "and violation_desc = 'STR CLEAN'\n",
    "union\n",
    "select 'Other' as violation_desc, sum(fine_amount::double) as sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "and violation_desc <> 'STR CLEAN';\"\"\"\n",
    "fines_clean_other_2023_df = duckdb.sql(query).to_df()\n",
    "fines_clean_other_2023_df[\n",
    "    \"fraction\"] = fines_clean_other_2023_df.sum_fines / fines_clean_other_2023_df.sum_fines.sum()\n",
    "fines_clean_other_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2621e7e7b2e295b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wow, over 42% of fines by violation are from street cleaning for a sum of over &#36;42M. That's a huge revenue stream just from street cleaning violations!\n",
    "\n",
    "Let's look at the violations per hour of the day, but let's also add in weekend vs weekday to see if there is any difference. We can:\n",
    "\n",
    "1. `cast()` the citation datetime to weekday to get day of the week\n",
    "2. Use a `case when` to populate a `varchar` with `weekday` or `weekend` accordingly\n",
    "3. Pull out the hour of the day\n",
    "4. `group by` the day of the week (type) and hour\n",
    "5. Aggregate the sum of fines in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88db7dd15014bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.347542Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "case when weekday(citation_issued_datetime::timestamp) in (0, 6) then 'weekend'\n",
    "else 'weekday' end as type,\n",
    "hour(citation_issued_datetime::timestamp) as hour,\n",
    "sum(fine_amount::double) sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "group by type, hour\n",
    "order by type, hour;\"\"\"\n",
    "fine_day_week_hour_2023_df = duckdb.sql(query).to_df()\n",
    "fine_day_week_hour_2023_df[\n",
    "    \"fraction\"] = fine_day_week_hour_2023_df.sum_fines / fine_day_week_hour_2023_df.sum_fines.sum()\n",
    "fine_day_week_hour_2023_df[\n",
    "    \"proportion\"] = fine_day_week_hour_2023_df.fraction / fine_day_week_hour_2023_df.fraction.max()\n",
    "fine_day_week_hour_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360843a32cab1446",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Interesting, just like the peak parking meter revenue, the peak parking citation revenue is also at 12 PM on a weekday!\n",
    "\n",
    "There are 48 rows of data here, so it would likely be much easier to see it visually\n",
    "\n",
    "Since we know it's a large portion of total fines, we could also break out by hour and street cleaning violations versus others for weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de750283b844c5f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.348815Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "case when weekday(citation_issued_datetime::timestamp) in (0, 6) then 'weekend'\n",
    "else 'weekday' end as type,\n",
    "case when violation_desc = 'STR CLEAN' then 'Street cleaning'\n",
    "else 'Other' end as cleaning_or_other,\n",
    "hour(citation_issued_datetime::timestamp) as hour,\n",
    "sum(fine_amount::double) sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "and weekday(citation_issued_datetime::timestamp) not in (0, 6)\n",
    "group by type, hour, cleaning_or_other\n",
    "order by type, hour, cleaning_or_other desc;\n",
    "\"\"\"\n",
    "clean_other_weekday_hour_2023_df = duckdb.sql(query).to_df()\n",
    "clean_other_weekday_hour_2023_df[\n",
    "    \"fraction\"] = clean_other_weekday_hour_2023_df.sum_fines / clean_other_weekday_hour_2023_df.sum_fines.sum()\n",
    "clean_other_weekday_hour_2023_df[\n",
    "    \"proportion\"] = clean_other_weekday_hour_2023_df.fraction / clean_other_weekday_hour_2023_df.fraction.max()\n",
    "clean_other_weekday_hour_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa100e0b0bb697eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Just glancing at these results, we can see the peak is street cleaning violations at 9 AM on a weekday with a sum of &#36;8.5M!\n",
    "\n",
    "Now let's do the same for weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f00eee8a5eb40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.350117Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "case when weekday(citation_issued_datetime::timestamp) in (0, 6) then 'weekend'\n",
    "else 'weekday' end as type,\n",
    "case when violation_desc = 'STR CLEAN' then 'Street cleaning'\n",
    "else 'Other' end as cleaning_or_other,\n",
    "hour(citation_issued_datetime::timestamp) as hour,\n",
    "sum(fine_amount::double) sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "and weekday(citation_issued_datetime::timestamp) in (0, 6)\n",
    "group by type, hour, cleaning_or_other\n",
    "order by type, hour, cleaning_or_other desc;\n",
    "\"\"\"\n",
    "clean_other_weekend_hour_2023_df = duckdb.sql(query).to_df()\n",
    "clean_other_weekend_hour_2023_df[\n",
    "    \"fraction\"] = clean_other_weekend_hour_2023_df.sum_fines / clean_other_weekend_hour_2023_df.sum_fines.sum()\n",
    "clean_other_weekend_hour_2023_df[\n",
    "    \"proportion\"] = clean_other_weekend_hour_2023_df.fraction / clean_other_weekend_hour_2023_df.fraction.max()\n",
    "clean_other_weekend_hour_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d4250cd31d563",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we can spot a peak on the weekend at 11 AM, and it's for anything but street cleaning with a sum just shy of $1M\n",
    "\n",
    "Maybe we've been too focused on street cleaning citations, let's look at all citations by total fines\n",
    "\n",
    "1. `group by` `violation` and `violation_desc`\n",
    "2. Aggregate the sum of the fines per group\n",
    "3. Filter results where sum is over our user variable threshold of &#36;1M\n",
    "4. Join the resulting DataFrame with a longer description defined above\n",
    "5. Calculate the fraction and proportion of each violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da9b17d339b35f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.351695Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "violation,\n",
    "violation_desc,\n",
    "sum(fine_amount::double) sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "group by violation, violation_desc\n",
    "having sum_fines > {sum_fines_min_thresh}\n",
    "order by sum_fines desc;\"\"\"\n",
    "fine_amounts_2023_df = duckdb.sql(query).to_df()\n",
    "fine_amounts_2023_df = fine_amounts_2023_df.merge(violation_df, left_on=\"violation_desc\", right_index=True)\n",
    "fine_amounts_2023_df[\n",
    "    \"fraction\"] = fine_amounts_2023_df.sum_fines / fine_amounts_2023_df.sum_fines.sum()\n",
    "fine_amounts_2023_df[\n",
    "    \"proportion\"] = fine_amounts_2023_df.fraction / fine_amounts_2023_df.fraction.max()\n",
    "fine_amounts_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb532a2f09160f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we know, the top violation is street cleaning. This is followed by residential parking violations, expired parking meters which are broken out by downtown and not, no plates, and yellow zone. The next violation isn't technically a parking citation at all, it's a fare evasion on SFMTA MUNI public transportation\n",
    "\n",
    "Next we can `group by` license plate to see the vehicles with the highest total fines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3922b61d08a889",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.352878Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "vehicle_plate_state,\n",
    "vehicle_plate,\n",
    "count(fine_amount) as count_fines,\n",
    "sum(fine_amount::double) as sum_fines\n",
    "from read_parquet('{data_dir}/*.parquet')\n",
    "where\n",
    "year(citation_issued_datetime::date) = 2023\n",
    "and vehicle_plate <> ''\n",
    "and fine_amount > 0\n",
    "group by vehicle_plate_state, vehicle_plate\n",
    "order by sum_fines desc;\"\"\"\n",
    "fine_plate_2023_df = duckdb.sql(query).to_df()\n",
    "fine_plate_2023_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e8e96c555d3fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It looks like there were citations for over 455k unique license plates\n",
    "\n",
    "Let's see the max total fines for a single vehicle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f620b83544319",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.353666Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fine_plate_2023_df.sum_fines.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffb24d6530c11e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wow, over &#36;18k in fines for a single vehicle. That's more than a fair number of vehicles are worth!\n",
    "\n",
    "What about the median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becf8878fce63a9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.354413Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fine_plate_2023_df.sum_fines.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de07cad674943e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The median fines per plate in the dataset is &#36;108. That is almost 1/200th of the maximum fines\n",
    "\n",
    "Finally, we can view every 100th item in the citations per license plate. This was used in creating a plot showing the distribution of fines per license plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fed1eb173883ac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.355131Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fine_plate_2023_df[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a90d09b3da57c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Well, this is all the data used to generate the *Parked 2023* infographic\n",
    "\n",
    "Nick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead305a9a28e7c4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T23:34:44.355802Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
